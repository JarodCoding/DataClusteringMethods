# Machine Learning Techniqus
  A collection of machine learning techniques. All the algorythems were created by me for educational purposes. I tried to make them as efficient and generic as possible but they might not suit your application. You are free to use them though
## K Means
Treats every oberservation as a d dimensional Vector. The algorythem generates clusters by assigning each observation to the cluster closest to its corresponding mean. And then recalculates the means simply by calculating the average positions of all vectors in the cluster. The first means are randomly selected point (In this implementation there is an algorythem that keeps the selected points within a certain distance from the average of all oberservations and also tries to keep the selcted means wihin this distance from each other. This algorythem was created to imporve results and performance it is still WIP though)
## KNN
Treats every oberservation as a d dimensional Vector. The training phase simply consists of supplying oberservations linked to classifications. When the algorythem is called to qualify x k nearest neighbours of x are selected. The final classification of x is calculated based on the classification assigned to these neighbours. The simplist way of achieving this is simply assining x the classification with the largest quantity of neighbours. This method can lead to quite unsatisfactory results though especially in case of the traing samples being unevenly distrivbuted among the classification and therefore I made the function which assigns a classification to x based on its k neighbours abstract. I have provided a class which impliments this function which selects the classification as eloborated above but you are free the impliment a differnt solution. I also provided the option of letting the algorithem automatically select k for you in that case the squareroot of the totalamount of trained data is used.
#SOM
Creates a map of the traing data. There is an abstract 2 dimensional map of n dimensional vectors (whose inital arrangenment is a parameter but it should either be very small ranomized values or in special cases be generatues using an algorythem specific to the data set). These n dimensional vectors are called weight vectors or neuros and are similar in function to means of the k means algorythem. The 2 dimensional map is static (the vertical and horizontal distance of every neuron to its neighbouring neurons is 1 and nothing about this 2 dimensional map every changes) Now when the map is trained using a dataset of n dimensional trainingvectors an derivetive of the k means algorythem is employed. The follwing is repeated s times (this is a parameter): For every trainingvector the closest weight vector is selected. Afterwards all vectors within a radius, which is calculated using a function exponentially decreasing from the half of the larger side of the grid at the first iteration to 1 at the last iteration, from the neuron with the weightvector closest to the traingvector are changed by differenz between the weight and the traing vector multiplied by a function which exponentialy decreasing proportional to the distance squared over the radius squared (so there is a decreasin amount of change the furtherout you go within the affected radius) and a function which is simply exponentially decreasin each iteration from a given start value (so there is a decreasin amount of changed)
